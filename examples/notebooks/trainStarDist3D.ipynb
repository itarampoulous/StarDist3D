{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of StarDist3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from stardist_tools import calculate_extents, Rays_GoldenSpiral\n",
    "from stardist_tools.matching import matching, matching_dataset\n",
    "from stardist_tools.csbdeep_utils import download_and_extract_zip_file\n",
    "\n",
    "\n",
    "from src.training import train\n",
    "from src.data.stardist_dataset import get_train_val_dataloaders\n",
    "from src.utils import seed_all, prepare_conf, plot_img_label\n",
    "from src.models.config import ConfigBase, Config3D\n",
    "from src.models.stardist3d import StarDist3D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters from configuration file\n",
    "You can use the cell below to load the parameters from a configuration file. Alternatively, you can manually define the parameters [here](#manually_define_parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML file\n",
    "with open('path/to/configuration_file/', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "assert str(config['Threshold_optimizer']['dataset']) in ['train', 'val'], \"No valid dataset has been provided for threshold optimization. Please determine whether to use the training ('train') or validation ('val') dataset for NMS threshold optimization.\"\n",
    "assert str(config['Threshold_optimizer']['epoch']) in ['last', 'best'], \"No valid epoch has been provided for threshold optimization. Please determine whether to use the 'best' or 'last' epoch for NMS threshold optimization.\"\n",
    "\n",
    "print(config)\n",
    "\n",
    "conf = Config3D(\n",
    "    name                           = config['Attributes']['name'],\n",
    "    random_seed                    = config['Attributes']['random_seed'],\n",
    "    \n",
    "    # ========================= Networks configurations ==================\n",
    "    init_type                       = config['Networks']['init_type'],\n",
    "    init_gain                       = config['Networks']['init_gain'],\n",
    "\n",
    "    backbone                        = config['Networks']['backbone'],\n",
    "    grid                            = config['Networks']['grid'],\n",
    "    anisotropy                      = config['Networks']['anisotropy'],\n",
    "\n",
    "    n_channel_in                   = config['Networks']['n_channel_in'],\n",
    "    kernel_size                    = config['Networks']['kernel_size'],\n",
    "    resnet_n_blocks                = config['Networks']['resnet_n_blocks'],\n",
    "    resnet_n_downs                 = config['Networks']['resnet_n_downs'],\n",
    "    n_filter_of_conv_after_resnet  = config['Networks']['n_filter_of_conv_after_resnet'],\n",
    "    resnet_n_filter_base           = config['Networks']['resnet_n_filter_base'],\n",
    "    resnet_n_conv_per_block        = config['Networks']['resnet_n_conv_per_block'],\n",
    "    use_batch_norm                  = config['Networks']['use_batch_norm'],\n",
    "\n",
    "    #======================================================================\n",
    "\n",
    "    # ========================= dataset ==================================\n",
    "    data_dir                       = r\"{}\".format(config['Dataset']['data_dir']),\n",
    "    val_size                       = config['Dataset']['val_size'],\n",
    "    \n",
    "    n_rays                         = config['Dataset']['Parameters']['n_rays'],\n",
    "\n",
    "    foreground_prob                = config['Dataset']['Parameters']['foreground_prob'],\n",
    "    n_classes                      = None,\n",
    "    patch_size                     = config['Dataset']['Parameters']['patch_size'],\n",
    "    cache_sample_ind               = config['Dataset']['Parameters']['cache_sample_ind'],\n",
    "    cache_data                     = config['Dataset']['Parameters']['cache_data'],\n",
    "\n",
    "    batch_size                     = config['Dataset']['batch_size'],\n",
    "    num_workers                    = config['Dataset']['num_workers'],\n",
    "\n",
    "    preprocess                     = config['Dataset']['preprocessing']['preprocess'],\n",
    "    preprocess_val                 = config['Dataset']['preprocessing']['preprocess_val'],\n",
    "    intensity_factor_range         = config['Dataset']['preprocessing']['intensity_factor_range'],\n",
    "    intensity_bias_range           = config['Dataset']['preprocessing']['intensity_bias_range'],\n",
    "\n",
    "    #======================================================================\n",
    "\n",
    "\n",
    "    # ========================= Training ==================================\n",
    "\n",
    "    use_gpu                        = True if torch.cuda.is_available()and config['Trainer']['use_gpu'] else False,\n",
    "    #gpu_ids                       = config['Trainer']['gpu_ids'],\n",
    "    use_amp                        = config['Trainer']['use_amp'],\n",
    "    isTrain                        = config['Trainer']['isTrain'] ,\n",
    "    evaluate                       = config['Trainer']['evaluate'],\n",
    "\n",
    "    \n",
    "    load_epoch                     = config['Trainer']['load_epoch'],\n",
    "    n_epochs                       = config['Trainer']['n_epochs'],\n",
    "    n_steps_per_epoch              = config['Trainer']['n_steps_per_epoch'],\n",
    "\n",
    "    save_epoch_freq                = config['Trainer']['save_epoch_freq'],\n",
    "    start_saving_best_after_epoch  = config['Trainer']['start_saving_best_after_epoch'],\n",
    "\n",
    "    lambda_prob                    = config['Trainer']['Parameters']['lambda_prob'],\n",
    "    lambda_dist                    = config['Trainer']['Parameters']['lambda_dist'],\n",
    "    lambda_reg                     = config['Trainer']['Parameters']['lambda_reg'],\n",
    "    lambda_prob_class              = config['Trainer']['Parameters']['lambda_prob_class'],\n",
    "\n",
    "    #======================================================================\n",
    "\n",
    "\n",
    "    # ========================= Optimizers ================================\n",
    "    lr                             = config['Optimizer']['lr'],\n",
    "    beta1                          = config['Optimizer']['beta1'],\n",
    "    beta2                          = config['Optimizer']['beta2'],\n",
    "\n",
    "    lr_policy                      = config['Optimizer']['lr_policy'],\n",
    "    lr_plateau_factor              = config['Optimizer']['lr_plateau_factor'],\n",
    "    lr_plateau_threshold           = config['Optimizer']['lr_plateau_threshold'],\n",
    "    lr_plateau_patience            = config['Optimizer']['lr_plateau_patience'],\n",
    "    min_lr                         = config['Optimizer']['min_lr'],\n",
    "    \n",
    "    lr_linear_n_epochs             = config['Optimizer']['lr_linear_n_epochs'],\n",
    "    lr_decay_iters                 = config['Optimizer']['lr_decay_iters'],\n",
    "    T_max                          = config['Optimizer']['T_max'])\n",
    "\n",
    "\n",
    "conf.checkpoints_dir = config['Attributes']['output_dir'] + '/checkpoints'\n",
    "conf.log_dir = config['Attributes']['output_dir'] + '/logs'\n",
    "conf.result_dir = config['Attributes']['output_dir'] + '/results'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually define parameters <a id='manually_define_parameters'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/directory/to/export/models/'\n",
    "\n",
    "conf = Config3D(\n",
    "    # Name to give to the model\n",
    "    name                           = 'Stadist3D_v.....',\n",
    "    \n",
    "    # Random seed to use for reproducibility\n",
    "    random_seed                    = 42,\n",
    "    \n",
    "    # ========================= Networks configurations ==================\n",
    "    init_type                       = 'normal',\n",
    "    init_gain                       = 0.02,\n",
    "\n",
    "    backbone                        = 'resnet',\n",
    "\n",
    "    # Subsampling factors (must be powers of 2) for each of the axes.\n",
    "    # Model will predict on a subsampled grid for increased efficiency and larger field of view.\n",
    "    grid                            = 'auto',\n",
    "\n",
    "    # Anisotropy on your imaging dataset\n",
    "    anisotropy                      = [4,1,1],\n",
    "\n",
    "    # Number of channel of images\n",
    "    n_channel_in                   = 1,\n",
    "\n",
    "    # Kernel size to use in neural network (use [3,3,3] for 3D)\n",
    "    kernel_size                    = [3,3,3],\n",
    "\n",
    "    # Number of ResNet blocks to use\n",
    "    resnet_n_blocks                = 4,\n",
    "\n",
    "    # None if grid = 'auto'. Otherwise, it has to match the grid.\n",
    "    resnet_n_downs                 = '',\n",
    "\n",
    "    # Number of filter in the convolution layer before the final prediction layer.\n",
    "    n_filter_of_conv_after_resnet  = 128,\n",
    "\n",
    "    # Number of filter to use in the first convolution layer\n",
    "    resnet_n_filter_base           = 32,\n",
    "\n",
    "    # Number of convolution layers to use in each ResNet block.\n",
    "    resnet_n_conv_per_block        = 3,\n",
    "    use_batch_norm                  = False,\n",
    "\n",
    "    #======================================================================\n",
    "\n",
    "    # ========================= dataset ==================================\n",
    "    # Path to datasets directory containing a daughter directory named 'train' with two daughter directories \n",
    "    # named 'images' and 'masks'\n",
    "    # Note that if  evaluate is set to True, it will validate during training using the data provided under datasets/val/ \n",
    "    data_dir                       = r'/path/to/datasets/' ,\n",
    "\n",
    "    # Fraction (0...1) of data from the `train` folder\n",
    "    # to use as validation set when the `val` folder doesn't exist\n",
    "    val_size                       = 0.15,\n",
    "    \n",
    "    # Number of rays to use in in the star-convex representation of nuclei shape \n",
    "    n_rays                         = 96,\n",
    "\n",
    "    # Fraction (0..1) of patches that will only be sampled from regions that contain foreground pixels.\n",
    "    foreground_prob                = 0.9,\n",
    "\n",
    "    n_classes                      = None,\n",
    "    \n",
    "    # Size of image to crop from original images\n",
    "    patch_size                     = [10,32,32],\n",
    "\n",
    "    # Set True to store indices of valid patches in RAM\n",
    "    cache_sample_ind               = True,\n",
    "\n",
    "    # Set True to store training data in RAM\n",
    "    cache_data                     = True,\n",
    "\n",
    "    # Size of batches\n",
    "    batch_size                     = 2,\n",
    "\n",
    "    # Number of subprocesses to use for data training.\n",
    "    num_workers                    = 0,\n",
    "\n",
    "    # Type of augmentation to do on training data.\n",
    "    # available augmentations: none|flip|randintensity\n",
    "    # You can use multiple of them, e.g. flip_randintensity\n",
    "    preprocess                     = \"flip_randintensity\",\n",
    "\n",
    "    # Type of augmentation to do on validation data.\n",
    "    preprocess_val                 = \"none\",\n",
    "\n",
    "    # Range from which to sample weight to multiply image intensities.\n",
    "    # Associated to `randintensity` augmentation.     \n",
    "    intensity_factor_range         = [0.6, 2.],\n",
    "\n",
    "    # Range from which to sample bias to add to image intentsities.\n",
    "    # Associated to `randintensity` augmentation. \n",
    "    intensity_bias_range           = [-0.2, 0.2],\n",
    "\n",
    "    #======================================================================\n",
    "\n",
    "\n",
    "    # ========================= Training ==================================\n",
    "    # Whether to use GPU\n",
    "    use_gpu                        = True if torch.cuda.is_available() else False,\n",
    "    #gpu_ids                       = [0],\n",
    "\n",
    "    # whether to use Automatic Mixed Precision\n",
    "    use_amp                        = True,\n",
    "\n",
    "    # Whether to initialize model in traning mode (set optimizers, schedulers ...)\n",
    "    isTrain                        = True,\n",
    "\n",
    "    # Whether to perform evaluation during traning.\n",
    "    evaluate                       = False,\n",
    "\n",
    "    # If not None, it will load state corresponding to epoch `load_epoch` and continue training from there\n",
    "    load_epoch                     = '',\n",
    "\n",
    "    # Number of training epochs\n",
    "    n_epochs                       = 400,\n",
    "\n",
    "    # Number of weights updates per epoch  \n",
    "    n_steps_per_epoch              = 100,\n",
    "\n",
    "    # Epoch saving frequency\n",
    "    save_epoch_freq                = 50,\n",
    "\n",
    "    # Epoch after which to start to save the best model\n",
    "    start_saving_best_after_epoch  = 5,\n",
    "\n",
    "    lambda_prob                    = 1.0,\n",
    "    lambda_dist                    = 0.2,\n",
    "    lambda_reg                     = 0.0001,\n",
    "    lambda_prob_class              = 1.0,\n",
    "\n",
    "    #======================================================================\n",
    "\n",
    "\n",
    "    # ========================= Optimizers ================================\n",
    "    # Learning rate\n",
    "    lr                             = 0.0002,\n",
    "\n",
    "    # Parameters for Adam optimizer\n",
    "    beta1                          = 0.9,\n",
    "    beta2                          = 0.999,\n",
    "\n",
    "    # Learning rate scheduler policy\n",
    "    # Possible values:\n",
    "    #       \"none\" -> keep the same learning rate for all epochs\n",
    "    #       \"plateau\" -> Pytorch ReduceLROnPlateau scheduler\n",
    "    #       \"linear_decay\" -> linearly decay learning rate from `lr` to 0\n",
    "    #       \"linear\" -> linearly increase  learning rate from 0 to `lr` during the first `lr_linear_n_epochs` and use `lr` for the remaining epochs\n",
    "    #       \"step\" -> reduce learning rate by 10 every `lr_step_n_epochs`\n",
    "    #       \"cosine\" -> Pytorch CosineAnnealingLR  \n",
    "    lr_policy                      = \"plateau\",\n",
    "\n",
    "    # Parameters when lr_policy = \"plateau\"\n",
    "    lr_plateau_factor              = 0.5,\n",
    "    lr_plateau_threshold           = 0.0000001,\n",
    "    lr_plateau_patience            = 40,\n",
    "    min_lr                         = 0.00000001,\n",
    "    \n",
    "    # Parameters when lr_policy = \"linear\"\n",
    "    lr_linear_n_epochs             = '',\n",
    "\n",
    "    # Parameters when lr_policy = \"linear_decay\"  \n",
    "    lr_decay_iters                 = '',\n",
    "\n",
    "    # T_max parameter of Pytorch CosineAnnealingLR when `lr_policy` = \"cosine\n",
    "    T_max                          = ''\n",
    "    )\n",
    "\n",
    "conf.checkpoints_dir = output_dir + '/checkpoints'\n",
    "conf.log_dir = output_dir + '/logs'\n",
    "conf.result_dir = output_dir + '/results'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train of StarDist3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(conf.random_seed)\n",
    "\n",
    "opt = prepare_conf(conf)\n",
    "\n",
    "model = StarDist3D(opt)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(\"Total number of epochs\".ljust(25), \":\", model.opt.n_epochs)\n",
    "\n",
    "fov = np.array( [max(r) for r in model._compute_receptive_field()] )\n",
    "object_median_size = opt.extents\n",
    "\n",
    "print(\"Median object size\".ljust(25), \":\", object_median_size)\n",
    "print(\"Network field of view\".ljust(25), \":\", fov)\n",
    "\n",
    "if any(object_median_size > fov):\n",
    "    warnings.warn(\"WARNING: median object size larger than field of view of the neural network.\")\n",
    "\n",
    "rays = Rays_GoldenSpiral(opt.n_rays, anisotropy=opt.anisotropy)\n",
    "\n",
    "train_dataloader, val_dataloader = get_train_val_dataloaders(opt, rays)\n",
    "\n",
    "total_nb_samples = len( train_dataloader.dataset ) + ( len(val_dataloader.dataset) if val_dataloader is not None else 0 )\n",
    "nb_samples_train = len(train_dataloader.dataset)\n",
    "nb_samples_val = total_nb_samples - nb_samples_train\n",
    "\n",
    "print(\"Total nb samples: \".ljust(40), total_nb_samples)\n",
    "print(\"Train nb samples: \".ljust(40), nb_samples_train)\n",
    "print(\"Val nb samples: \".ljust(40), nb_samples_val)\n",
    "\n",
    "print(\"Train augmentation\".ljust(25), \":\",  train_dataloader.dataset.opt.preprocess)\n",
    "print(\"Val augmentation\".ljust(25), \":\", val_dataloader.dataset.opt.preprocess)\n",
    "\n",
    "train(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS threshold optimization\n",
    "This can be performed in either the train or the validation dataset:\n",
    "- [Optimize based on the training dataset](#train_dataset_optimization).\n",
    "- [Optimize based on the validation dataset](#val_dataset_optimization)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization based on the training dataset <a id='train_dataset_optimization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(config['Threshold_optimizer']['dataset']) == 'train':\n",
    "    print(\"Using training dataset for NMS threshold optimization:\")\n",
    "    X, Y = train_dataloader.dataset.get_all_data()\n",
    "    conf.load_epoch = conf.n_epochs\n",
    "    model = StarDist3D(conf)\n",
    "\n",
    "if str(config['Threshold_optimizer']['dataset']) == 'val':\n",
    "    print(\"Using validation dataset for NMS threshold optimization:\")\n",
    "    X, Y = val_dataloader.dataset.get_all_data()\n",
    "    \n",
    "    if str(config['Threshold_optimizer']['epoch'])== 'best':\n",
    "        print(\"Optimizing thresholds for best model...\")\n",
    "        conf.load_epoch = \"best\"\n",
    "\n",
    "    elif str(config['Threshold_optimizer']['epoch'])== 'last': \n",
    "        print(\"Optimizing thresholds for last model...\")\n",
    "        conf.load_epoch = conf.n_epochs\n",
    "    \n",
    "    model = StarDist3D(conf)\n",
    "\n",
    "if (config['Threshold_optimizer']['NMS_thresh'] is not None and config['Threshold_optimizer']['IoU_thresh'] is not None):\n",
    "    print('Using default settings for NMS threshold optimization...')\n",
    "    model.optimize_thresholds(X, Y, \n",
    "                                nms_threshs = config['Threshold_optimizer']['NMS_thresh'],\n",
    "                                iou_threshs = config['Threshold_optimizer']['IoU_thresh'])\n",
    "else:\n",
    "    model.optimize_thresholds(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization based on the validation dataset <a id='val_dataset_optimization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StarDist3D_gh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
